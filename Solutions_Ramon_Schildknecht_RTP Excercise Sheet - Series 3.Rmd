---
title: "Solutions_Ramon_Schildknecht_RTP Excercise Sheet - Series 3"
author: "[Ramon Schildknecht](https://www.linkedin.com/in/ramon-schildknecht-36301756/)"
output:
  html_document:
    df_print: paged
    toc: true
    theme: united
  word_document: default
---

<br>
<br>

```{r message=FALSE, warning=FALSE}
library(tseries)
library(tidyverse)
library(magrittr)
library(plotly)
library(timetk)
library(lubridate)
library(pracma)
library(fma)
library(forecast)
```

<br>
<br>

# Series 3

<br>

## Exercise 3.1 - simulations are key

i)
AR(2): PACF should show a cut-off at lag p = 2
<br>
MA(3): ACF should show a cut-off at lag q = 3


ii)

AR(2) model

```{r}
# AR2 model - theoretical autocorrelations
plot(0:30, ARMAacf(ar = c(0.9, -0.5), lag.max = 30), type = "h",
ylab = "ACF")

## Theoretical partial autocorrelations
plot(1:30, ARMAacf(ar = c(0.9, -0.5), lag.max = 30, pacf = TRUE), type = "h", ylab = "PACF")

```

MA(3) model

```{r}
# MA3 model - theoretical autocorrelations
plot(0:30, ARMAacf(ma = c(0.9, -0.5, -0.4), lag.max = 30), type = "h",
ylab = "ACF")

## Theoretical partial autocorrelations
plot(1:30, ARMAacf(ma = c(0.9, -0.5, -0.4), lag.max = 30, pacf = TRUE), type = "h", ylab = "PACF")

```


iii)

AR(2) process

```{r}
# define 3 data sets
set.seed(22)
ar2_1_sim <- arima.sim(n = 200, model = list(ar = c(0.9, -0.5)))
ar2_2_sim <- arima.sim(n = 200, model = list(ar = c(0.9, -0.5)))
ar2_3_sim <- arima.sim(n = 200, model = list(ar = c(0.9, -0.5)))

xdata <- 1:200

plot(xdata, ar2_1_sim, type="l", col="blue", lty=1, ylim=c(-5,5), ylab="values" )
lines(xdata, ar2_2_sim, col="red",lty=2)
lines(xdata, ar2_3_sim, col="green",lty=5)
```
Observations: one sees clear recurring lows and peaks in all 3 realisations. The values are between -5 and 5.


MA(3) process

Just used a length of 52 for better pattern examination.

```{r}
# define 3 data sets
set.seed(22)
ma3_1_sim <- arima.sim(n = 52, model = list(ma = c(0.9, -0.5, -0.4)))
ma3_2_sim <- arima.sim(n = 52, model = list(ma = c(0.9, -0.5, -0.4)))
ma3_3_sim <- arima.sim(n = 52, model = list(ma = c(0.9, -0.5, -0.4)))

xdata <- 1:52

plot(xdata, ma3_1_sim, type="l", col="blue", lty=1, ylim=c(-5,5), ylab="values" )
lines(xdata, ma3_2_sim, col="red",lty=2)
lines(xdata, ma3_3_sim, col="green",lty=5)
```

Observations: one sees clear recurring lows and peaks in all 3 realisations which are also close to each other over all 3 realisations. The values are between -4 and 4. 

iv)

AR(2) process

```{r}
set.seed(22)
ar2_1_sim <- arima.sim(n = 200, model = list(ar = c(0.9, -0.5)))
time = 1:200
plot(time, ar2_1_sim, type="l", col="blue", lty=1, ylim=c(-5,5), ylab="values" )
acf(ma3_1_sim)
pacf(ma3_1_sim)
```

Obervations: ACF shows clear cut off after lag 2. PACF shows clear cut off after lag 2. This is a similar result as seen in the simulated data above.

<br>

MA(3) process

Just used a length of 52 for better pattern examination.

```{r}
set.seed(22)
ma3_1_sim <- arima.sim(n = 52, model = list(ma = c(0.9, -0.5, -0.4)))
time = 1:52
plot(time, ma3_1_sim, type="l", col="blue", lty=1, ylim=c(-4,4), ylab="values" )
acf(ma3_1_sim)
pacf(ma3_1_sim)
```

Obervations: ACF shows clear cut off after lag 2. PACF shows clear cut off after lag 1. ACF shows a similar result as seen in the simulated data above. PACF shows quite a different result.

<br>


## Exercise 3.2 - Stationarity of AR(p) models


a)

i)

```{r}
polyroot(c(1, -0.5, -2))
```

As the first value has an absolute value below 1, the time series is not stationary.

ii)

```{r}
polyroot(c(1, -1))
```

As the value has an absolute value of 1, the time series could be stationary.

b)
```{r}
values <- seq(-1000, 1000, 100)
for (i in values) {
  print(paste(i, polyroot(c(1, -0.5, i)))
  )
}
```

As the first value has an absolute value below 1 for alpha2 from -1000 to 1000 in intervalls of 100, the time series is could never be stationary with alpha1 of -0.5.

c) 

```{r}
# ACF
# ta<-ARMAacf(ar=(1),lag.max=22)
# plot(ta, type="h")
# 
# # PACF
# tp<-ARMAacf(ar=(1),pacf=TRUE)
# plot(tp, type="h")

values <- seq(0, 2, 0.5)
for (i in values) {
  ta<-ARMAacf(ar=(i),lag.max=22)
  plot(ta, type="h")
  tp<-ARMAacf(ar=(i),pacf=TRUE)
  plot(tp, type="h")
  print(paste(i, polyroot(c(1, -1, i)))
  )
}
```

Because Yt - 1 * alpha (>= 1) is equal or bigger than Yt. The characteristic polynomial is therefore below 1 and not stationary! The plots above as well as the polyroots confirm this statement.

<br>

## Exercise 3.3 - AR(p) models with yield data

a)

```{r}
# load data
yields <- read.table("http://stat.ethz.ch/Teaching/Datasets/WBL/yields.dat",
header = FALSE)
t_yields <- ts(yields[, 1])
hist(t_yields)
tsdisplay(t_yields)

```

Yes, the data could be generated by an AR-process of order p = 1. One can observe only a autocorrelation of lag 1 within the PACF plot. 


b)

```{r}
# unable to compute mean and innovation variance by hand. How is the suggested approach?
r_yw <- ar(t_yields, method = "yw", order.max = 1)
r_yw$resid
str(r_yw)
```

c)

```{r}
r_burg <- ar(t_yields, method = "burg", order.max = 1)
r_burg$resid
str(r_burg)
checkresiduals(r_burg$resid)
```

Residuals seem to be allright. No autocorrelation is detected and the residuals look normally distributed.

<br>

d)

```{r}
r_mle <- arima(t_yields, order = c(1, 0, 0), include.mean = TRUE)
str(r_mle)
r_mle
# calculate confidence intervalls
print("confidence intervals: ")
(ci_ar1_lower_band <- -0.4191-0.1129)
(ci_ar1_upper_band <- -0.4191+0.1129)
(ci_intercept_lower_band <- 51.2658-0.9137)
(ci_intercept_upper_band <- 51.2658+0.9137)

```

Confidence intervall for coefficient ar1 with mean -0.4191 is from -0.532 to  -0.3062.
Confidence intervall for coefficient intercept with mean 51.2658 is from 50.3521 to 52.1795.

<br>

## Exercise 3.4 - ARIMA

Load data

```{r}
d_force <- read.table("http://stat.ethz.ch/Teaching/Datasets/WBL/kraft.dat",
header = FALSE)
ts_force <- ts(d_force[, 1])
plot(ts_force)
```


a)

```{r}
ts_forceA <- window(ts_force, end = 280)
tsdisplay(ts_forceA)
acf(ts_forceA)
```

Yes. The expected behaviour is a lag at every multiple of about 2 seconds multiplied by 1.5 seconds of measurements. E. g. at lag 4 (6 seconds) or lag 8 (12 seconds).

The ACF plots confirms the expected behaviour. 

<br>

b)

```{r}
# PACF
pacf(ts_forceA) # cut-off at p = 6

# ar(6) model
ar_force <- ar(ts_forceA, method = "mle")
print("mle-method:")
ar_force$aic
which.min(ar_force$aic)


ar_force2 <- ar(ts_forceA, method = "burg")
print("burg-method:")
ar_force2$aic
which.min(ar_force2$aic)

ar_force3 <- ar(ts_forceA, method = "yw")
print("yw-method:")
ar_force3$aic
which.min(ar_force2$aic)

# tried CSS-ML as well
# lags <- seq(1,16)
# for (i in lags) {
#   fit_MLE <- arima(ts_forceA, order = c(i, 0, 0), method = "css-mle")
#   cat("AR(", i, "):", sep = "")
#   print(fit_MLE$aic)
# }
 
```

Maximum likelihood estimator: best lag = 10
Burg's algorithm & Yule-Walker equations: best lag = 17

<br>

c)

```{r}
ar_lme <- arima(ts_forceA, order = c(10, 0, 0), method = "ML")
summary(ar_lme)

# select residuals and display them
residuals <- ar_lme$residuals
tsdisplay(residuals)

qqnorm(ar_lme$residuals,main="Normal Q-Q Plot: AR(10)")
qqline(ar_lme$residuals)
```

Residuals looking quite nice. However there seems to be a autocorrelation at lag 8 which must be further analysed. Q-Q plot shows a approximation to normal distribution. The model is in general appropriate for this time series. One can observe from the results in b) that a Yule Walker equation model with lag 17 could even show better results than this model. 

<br>

d)

```{r}
force_pred <- predict(ar_force, n.ahead = 40)
plot(ts_force, lty = 3)
lines(ts_forceA, lwd = 2)
lines(force_pred$pred, lwd = 2, col = "orange")
lines(force_pred$pred + force_pred$se * 1.96, col = "red")
lines(force_pred$pred - force_pred$se * 1.96, col = "red")
```

The model predicts the values quite well, because the original data is between the confidence interval values. 

<br>
<br>


